{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (to be done only once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some preprocessing and save the preprocessed data into a separate file. Need to execute only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['command', 'label']\n",
    "df = pd.read_csv('./Utilities/Datasets/home_appliance_commands.csv', header=None, names=col_names)\n",
    "\n",
    "# Convert all sentences of command into lowercase\n",
    "commands = [item.lower() for item in df['command']]\n",
    "df['command'] = commands\n",
    "\n",
    "# Save to a file\n",
    "df.to_csv('./Utilities/Datasets/Preprocessed/home_appliance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remove the stop words from the dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the stop words from a string\n",
    "def remove_stop_words(string, stop_words):\n",
    "    new_string = ''\n",
    "    for word in string.split():\n",
    "        if word not in stop_words:\n",
    "            new_string += word + ' '\n",
    "            \n",
    "    return new_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the stop words from the file ./Utilities/Preprocessing Utilities/stop_words.txt\n",
    "with open('./Utilities/Preprocessing Utilities/stop_words.txt', 'r') as f:\n",
    "    stop_words = f.readlines()\n",
    "    stop_words = [word.replace('\\n', '') for word in stop_words]\n",
    "    \n",
    "# Remove the stop words from the dataset\n",
    "df = pd.read_csv('./Utilities/Datasets/Preprocessed/home_appliance.csv')\n",
    "df['command'] = df['command'].apply(lambda x: remove_stop_words(x, stop_words))\n",
    "\n",
    "# Save the DataFrame to a file\n",
    "df.to_csv('./Utilities/Datasets/Preprocessed/home_appliance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vector learning by FastTex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.train_unsupervised('./Utilities/Datasets/Preprocessed/home_appliance.csv', dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the word vectors in a file to retrieve them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model into a binary file (Folder should be already present)\n",
    "ft_model.save_model('./Result/home_appliance1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the learned vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft_model = fasttext.load_model('./Result/home_appliance1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dictionary data for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>light</th>\n",
       "      <th>geyser</th>\n",
       "      <th>off</th>\n",
       "      <th>on</th>\n",
       "      <th>water</th>\n",
       "      <th>hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batti</td>\n",
       "      <td>heater</td>\n",
       "      <td>band</td>\n",
       "      <td>chalu</td>\n",
       "      <td>paani</td>\n",
       "      <td>garam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bulb</td>\n",
       "      <td>machiniya</td>\n",
       "      <td>bujha</td>\n",
       "      <td>jalaiye</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>balab</td>\n",
       "      <td>machine</td>\n",
       "      <td>dark</td>\n",
       "      <td>jalaw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bujhaw</td>\n",
       "      <td>kara</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bujhaiye</td>\n",
       "      <td>jala</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   light     geyser       off       on  water    hot\n",
       "0  batti     heater      band    chalu  paani  garam\n",
       "1   bulb  machiniya     bujha  jalaiye    NaN    NaN\n",
       "2  balab    machine      dark    jalaw    NaN    NaN\n",
       "3    NaN        NaN    bujhaw     kara    NaN    NaN\n",
       "4    NaN        NaN  bujhaiye     jala    NaN    NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_dict = pd.read_csv('./Utilities/Translations/translations_data.csv')\n",
    "trans_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom dictionary to hold the vectors learnt by fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = dict((word, ft_model.get_word_vector(word)) for word in ft_model.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace the word vectors\n",
    "def replace_word_vec(main_dict, word_list, root_word):\n",
    "    for word in word_list:\n",
    "        main_dict[word] = ft_model.get_word_vector(root_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the word vectors for the words in the translation dictionary to get perfect similarity\n",
    "for indx, col in trans_dict.iteritems():\n",
    "    replace_word_vec(word_vecs, col, indx)\n",
    "    \n",
    "# Add the root words to the custom word_vector dictionary\n",
    "for word in trans_dict.columns:\n",
    "    word_vecs[word] = ft_model.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Save the new dictionary into a file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "wordVecDict = {'wordVecDict': word_vecs}\n",
    "\n",
    "with open('./Utilities/Result Vectors/word_vectors.bin', 'wb') as f:\n",
    "    f.write(pickle.dumps(wordVecDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Read the word vectors from the file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Utilities/Result Vectors/word_vectors.bin', 'rb') as f:\n",
    "    word_vecs = pickle.load(f)\n",
    "    word_vecs = word_vecs['wordVecDict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sentence vectors by simple averaging of word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no api in the fastText library to calculate the sentence vectors, we will do a custom implementation of the same by averaging the word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_norm(word_vec):\n",
    "    return np.sqrt(np.sum(word_vec ** 2))\n",
    "\n",
    "def get_l2_normed_vector(word_vec):\n",
    "    l2_norm = get_l2_norm(word_vec)\n",
    "    if l2_norm > 0:\n",
    "        return word_vec * (1.0 / l2_norm)\n",
    "    else:\n",
    "        return word_vec\n",
    "\n",
    "def get_sentence_vector(sentence):\n",
    "    sentence_vector = np.zeros(100)  # because the word vectors are 100-dimentional\n",
    "    for word in sentence.split():\n",
    "        word_vec = word_vecs[word]\n",
    "        sentence_vector = np.add(sentence_vector, get_l2_normed_vector(word_vec))\n",
    "        \n",
    "    return np.divide(sentence_vector, len(sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "# Read in the preprocessed dataset\n",
    "df = pd.read_csv('./Utilities/Datasets/Preprocessed/home_appliance.csv')\n",
    "\n",
    "# Get sentence vectors for each command and add them to a dictionary\n",
    "sentence_vector = dict()\n",
    "for sent in df['command']:\n",
    "    sentence_vector[sent] = get_sentence_vector(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Save the sentence vector dictionary to a file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Utilities/Result Vectors/sentence_vectors.bin', 'wb') as f:\n",
    "    sentDict = {'sentDict': sentence_vector}\n",
    "    f.write(pickle.dumps(sentDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Retrieve the sentence vectors </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Utilities/Result Vectors/sentence_vectors.bin', 'rb') as f:\n",
    "    sentence_vector = pickle.load(f)\n",
    "    sentence_vector = sentence_vector['sentDict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_vec'] = df['command'].apply(lambda sent: sentence_vector[sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Use LabelEncoder to transform the labels to numericalvalues </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Split the dataset </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[-2]]\n",
    "y = df[df.columns[-1]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Train the KNN Classifier </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.fit(X_train.tolist(), y_train.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Predict the results for the test data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_clf.predict(X_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get the results </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.tolist(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test.tolist(), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_clr = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_clr.fit(X_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_km = km_clr.predict(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 3, 1, 2, 0, 1, 2], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_km"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
